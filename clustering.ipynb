pip install simple-salesforce


#Load the required packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from simple_salesforce import Salesforce

#Plot styling
import seaborn as sns; sns.set()  # for plot styling
%matplotlib inline
sf = Salesforce(username='salesforceusername', password='password', security_token='securitytoken')
datais = sf.query("SELECT Id, Income__c,spend__c,cluster__c FROM Customers__c")

plt.rcParams['figure.figsize'] = (16, 9)
plt.style.use('ggplot')
#Read the csv file

df =   pd.DataFrame.from_dict(dict(datais)['records'])
data = df.drop(['attributes','Id','cluster__c'],axis=1) 
datasetOriginal =df#pd.read_csv('MOCK_DATA.csv')
dataset =data #datasetOriginal.drop(['first_name','email'],axis=1)
#Explore the dataset
dataset.head()#top 5 columns
len(dataset) # of rows
#descriptive statistics of the dataset
dataset.describe().transpose()



#Visualizing the data - displot
plot_income = sns.distplot(dataset["Income__c"])
plot_spend = sns.distplot(dataset["spend__c"])
plt.xlabel('Income / spend')


#Violin plot of Income and Spend
f, axes = plt.subplots(1,2, figsize=(12,6), sharex=True, sharey=True)
v1 = sns.violinplot(data=dataset, x='Income__c', color="skyblue",ax=axes[0])
v2 = sns.violinplot(data=dataset, x='spend__c',color="lightgreen", ax=axes[1])
v1.set(xlim=(0,420))



#Using the elbow method to find the optimum number of clusters
from sklearn.cluster import KMeans
wcss = []
for i in range(1,11):
    km=KMeans(n_clusters=i,init='k-means++', max_iter=300, n_init=10, random_state=0)
    km.fit(dataset)
    wcss.append(km.inertia_)
plt.plot(range(1,11),wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('wcss')
plt.show()



import json
import pickle 
from sklearn.externals import joblib 
X = dataset
##Fitting kmeans to the dataset with k=4
km4=KMeans(n_clusters=4,init='k-means++', max_iter=300, n_init=10, random_state=0)
ymodel = km4.fit(dataset) 
y_means = km4.fit_predict(dataset)

# Save the model as a pickle in a file 
joblib.dump(ymodel, 'filename.pkl') 

  
# Save the trained model as a pickle string. 
saved_model = pickle.dumps(ymodel) 
print(saved_model)
#Visualizing the clusters for k=4
#plt.scatter(X[y_means==0,0],X[y_means==0,1],s=50, c='purple',label='Cluster1')
#plt.scatter(X[y_means==1,0],X[y_means==1,1],s=50, c='blue',label='Cluster2')
#plt.scatter(X[y_means==2,0],X[y_means==2,1],s=50, c='green',label='Cluster3')
#plt.scatter(X[y_means==3,0],X[y_means==3,1],s=50, c='cyan',label='Cluster4')
#plt.scatter(km4.cluster_centers_[:,0], km4.cluster_centers_[:,1],s=200,marker='s', c='red', alpha=0.7, label='Centroids')
#plt.title('Customer segments')
#plt.xlabel('Annual income of customer')
#plt.ylabel('Annual spend from customer on site')
#plt.legend()
#plt.show()
datasetOriginal['Cluster__c'] = y_means
dataFinalIs = datasetOriginal.drop(['attributes'],axis=1)

#print(dataFinalIs)
finaljson = [] 

for index,row in dataFinalIs.iterrows():
    jsonIs = {}
   # print(row['Id'])
    jsonIs["Id"]=row['Id']
    jsonIs["Cluster__c"]=row['Cluster__c']
    finaljson.append(jsonIs)
    
#print(finaljson)
dataAtt  = finaljson

sf.bulk.Customers__c.update(dataAtt)


knn_from_joblib = joblib.load('filename.pkl')  
dataf = [[1500, 10],[100, 100],[1500, 1000],[10681,482],[12108,876]] 
# Create the pandas DataFrame 
dff = pd.DataFrame(dataf, columns = ['Income__c', 'spend__c']) 
knn_from_joblib.predict(dff)
